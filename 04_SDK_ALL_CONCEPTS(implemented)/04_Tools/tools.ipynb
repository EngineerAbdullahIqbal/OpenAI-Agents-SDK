{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd4d9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U openai-agents PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c6d0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeda857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv , find_dotenv # For Loading environment variables\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "OPENROUTER_API_KEY=os.getenv('OPENROUTER_API_KEY')\n",
    "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "MODEL = \"mistralai/mistral-small-3.1-24b-instruct:free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f1c620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent , Runner , AsyncOpenAI , OpenAIChatCompletionsModel \n",
    "from agents.tool import function_tool\n",
    "from agents.run import RunConfig\n",
    "\n",
    "external_client = AsyncOpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=BASE_URL,\n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\n",
    "    model=MODEL,\n",
    "    openai_client=external_client,\n",
    ")\n",
    "\n",
    "config = RunConfig(\n",
    "    model=model,\n",
    "    model_provider=external_client,\n",
    "    tracing_disabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e08b377d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool(\"get_weather\")\n",
    "def get_weather(location: str, unit: str = \"C\") -> str:\n",
    "  \"\"\"\n",
    "  Fetch the weather for a given location, returning a short description.\n",
    "  \"\"\"\n",
    "  # Example logic\n",
    "  return f\"The weather in {location} is 22 degrees {unit}.\"\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"weather_agent\",\n",
    "    instructions=\"Answer about weather update use get_weather tool and if user ask about temperature use get_temperature tool\",\n",
    "    tools=[get_weather, get_temperature],\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e6de229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 degrees C in Karachi is considered warm. It is generally classified as a hot and humid climate. The city experiences high temperatures throughout the year, with the summer months (June to September) being particularly hot, often exceeding 35째C (95째F). Winters are mild, with temperatures rarely dropping below 10째C (50째F). The weather in Karachi is typically characterized by high humidity, which can make the heat feel more intense.is there anything else i can help you with?\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(agent , input=\"weather in karachi?\" , run_config=config)\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "352320ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool(\"get_card\")\n",
    "def get_card(card_number: str) -> str:\n",
    "\n",
    "    # Example logic\n",
    "    student1 = \"Abdullah\",\n",
    "    student2 = \"Ahmed\",\n",
    "    student3 = \"Alim\",\n",
    "    student4 = \"Haseeb\",\n",
    "\n",
    "    card = f\"\"\"\"\n",
    "    Card Number: {card_number}\n",
    "    Student 1: {student1}\n",
    "    father_name: \"Mhammad Iqbal\"\n",
    "    Student 2: {student2}\n",
    "    father_name: \"Mhammad Iqbal\"\n",
    "    Student 3: {student3}\n",
    "    father_name: \"Mhammad Iqbal\"\n",
    "    Student 4: {student4}\n",
    "    father_name: \"Mhammad Iqbal\"\n",
    "    \"\"\"\n",
    "    return card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e2312",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'No auth credentials found', 'code': 401}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m agent = Agent(\n\u001b[32m      2\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mcard_agent\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     instructions=\u001b[33m\"\u001b[39m\u001b[33manswer about card use get_card tool\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     tools=[get_card],\n\u001b[32m      5\u001b[39m     model=model\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent , \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mget card 1\u001b[39m\u001b[33m\"\u001b[39m , run_config=config)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\run.py:218\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    213\u001b[39m logger.debug(\n\u001b[32m    214\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    219\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    220\u001b[39m             starting_agent,\n\u001b[32m    221\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    222\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    223\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    224\u001b[39m             context_wrapper,\n\u001b[32m    225\u001b[39m         ),\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    227\u001b[39m             agent=current_agent,\n\u001b[32m    228\u001b[39m             all_tools=all_tools,\n\u001b[32m    229\u001b[39m             original_input=original_input,\n\u001b[32m    230\u001b[39m             generated_items=generated_items,\n\u001b[32m    231\u001b[39m             hooks=hooks,\n\u001b[32m    232\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    233\u001b[39m             run_config=run_config,\n\u001b[32m    234\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    235\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    236\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    237\u001b[39m         ),\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    240\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    241\u001b[39m         agent=current_agent,\n\u001b[32m    242\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    250\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    251\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\run.py:756\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    753\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    754\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m756\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    757\u001b[39m     agent,\n\u001b[32m    758\u001b[39m     system_prompt,\n\u001b[32m    759\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    760\u001b[39m     output_schema,\n\u001b[32m    761\u001b[39m     all_tools,\n\u001b[32m    762\u001b[39m     handoffs,\n\u001b[32m    763\u001b[39m     context_wrapper,\n\u001b[32m    764\u001b[39m     run_config,\n\u001b[32m    765\u001b[39m     tool_use_tracker,\n\u001b[32m    766\u001b[39m     previous_response_id,\n\u001b[32m    767\u001b[39m )\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    770\u001b[39m     agent=agent,\n\u001b[32m    771\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    781\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\run.py:915\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    912\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    913\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    916\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    917\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    918\u001b[39m     model_settings=model_settings,\n\u001b[32m    919\u001b[39m     tools=all_tools,\n\u001b[32m    920\u001b[39m     output_schema=output_schema,\n\u001b[32m    921\u001b[39m     handoffs=handoffs,\n\u001b[32m    922\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    923\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    924\u001b[39m     ),\n\u001b[32m    925\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    926\u001b[39m )\n\u001b[32m    928\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:119\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    104\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     previous_response_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    112\u001b[39m ) -> ModelResponse:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m    114\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m    115\u001b[39m         model_config=dataclasses.asdict(model_settings)\n\u001b[32m    116\u001b[39m         | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m    117\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m    118\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    120\u001b[39m             system_instructions,\n\u001b[32m    121\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    122\u001b[39m             model_settings,\n\u001b[32m    123\u001b[39m             tools,\n\u001b[32m    124\u001b[39m             output_schema,\n\u001b[32m    125\u001b[39m             handoffs,\n\u001b[32m    126\u001b[39m             span_generation,\n\u001b[32m    127\u001b[39m             tracing,\n\u001b[32m    128\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    129\u001b[39m         )\n\u001b[32m    131\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    132\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mReceived model response\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:535\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[39m\n\u001b[32m    529\u001b[39m store = _Converter.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    531\u001b[39m stream_options = _Converter.get_stream_options_param(\n\u001b[32m    532\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    533\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    536\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    537\u001b[39m     messages=converted_messages,\n\u001b[32m    538\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    539\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    540\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    541\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    542\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    543\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    544\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    545\u001b[39m     response_format=response_format,\n\u001b[32m    546\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    547\u001b[39m     stream=stream,\n\u001b[32m    548\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    549\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    550\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    551\u001b[39m     extra_headers=_HEADERS,\n\u001b[32m    552\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    553\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    554\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    555\u001b[39m )\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2032\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1989\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1990\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1991\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2029\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2030\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2031\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2032\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2033\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2034\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2035\u001b[39m             {\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2063\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2064\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2065\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2066\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2067\u001b[39m             },\n\u001b[32m   2068\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2069\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2070\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2071\u001b[39m         ),\n\u001b[32m   2072\u001b[39m         options=make_request_options(\n\u001b[32m   2073\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2074\u001b[39m         ),\n\u001b[32m   2075\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2076\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2077\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2078\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1805\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1792\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1793\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1800\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1801\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1802\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1803\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1804\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1495\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1493\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1496\u001b[39m     cast_to=cast_to,\n\u001b[32m   1497\u001b[39m     options=options,\n\u001b[32m   1498\u001b[39m     stream=stream,\n\u001b[32m   1499\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1500\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1501\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ABDULLAH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1600\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1597\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1599\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1603\u001b[39m     cast_to=cast_to,\n\u001b[32m   1604\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1608\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1609\u001b[39m )\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'No auth credentials found', 'code': 401}}"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    name=\"card_agent\",\n",
    "    instructions=\"answer about card use get_card tool\",\n",
    "    tools=[get_card],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "result = await Runner.run(agent , input=\"get card 1\" , run_config=config)\n",
    "print(result.final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
